{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarielaNina/-Guide-to-Advanced-LLM-Techniques-Public/blob/main/M%C3%B3dulo_3_Ensembles_de_LLMs_A_Sabedoria_das_Multid%C3%B5es.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 3: Ensembles de LLMs - A Sabedoria das Multidões"
      ],
      "metadata": {
        "id": "wg-qHhguTI8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução"
      ],
      "metadata": {
        "id": "nCz1fxpITKUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos módulos anteriores, focamos em como aprimorar a interação com um único modelo através da engenharia de prompt. Agora, vamos explorar uma técnica poderosa emprestada do aprendizado de máquina tradicional: os ensembles.\n",
        "\n",
        "A ideia central de um ensemble é que \"várias cabeças pensam melhor que uma\". Em vez de confiar na resposta de um único modelo, combinamos as predições de múltiplos \"especialistas\" para chegar a uma decisão final mais robusta e precisa. LLMs, apesar de seu poder, podem ser suscetíveis a vieses, erros factuais ou \"alucinações\". Usar um ensemble pode mitigar esses riscos e aumentar a confiabilidade geral do sistema.\n",
        "\n",
        "Neste módulo, vamos explorar duas estratégias de ensemble, conforme detalhado no artigo:\n",
        "1. Votação Majoritária: A forma mais simples de ensemble, onde consultamos vários especialistas e adotamos a resposta da maioria.\n",
        "2. Negociação: Uma abordagem mais avançada que simula um debate estruturado entre modelos para refinar ideias e chegar a um consenso."
      ],
      "metadata": {
        "id": "X8eAMCj1TNBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Votação Majoritária"
      ],
      "metadata": {
        "id": "uiqIvXByTQ1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A votação é a forma mais intuitiva de ensemble. A ideia é consultar vários \"especialistas\" independentes e adotar a resposta da maioria. No contexto de LLMs, esses especialistas podem ser:\n",
        "1. Diferentes Modelos: Você pode fazer a mesma pergunta para o Llama 3, Mixtral e Sabiá, e depois contar os votos de cada um.\n",
        "2. Diferentes Prompts: Usar vários prompts (ex: um Zero-Shot, um Few-Shot) com o mesmo modelo.\n",
        "3. Múltiplas Execuções do Mesmo Modelo: Esta é a abordagem conhecida como Self-Consistency (Autoconsistência), proposta por Wang et al. (2022) [1]. Executamos o mesmo prompt várias vezes com uma temperatura > 0 para gerar respostas diversas e escolhemos a mais frequente.\n",
        "\n",
        "Vamos implementar a abordagem de Self-Consistency, pois é a mais prática quando se tem acesso a uma única API."
      ],
      "metadata": {
        "id": "Dw-_YBJaTVDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/Assaoka/Guide-to-Advanced-LLM-Techniques/blob/main/Imagens/sc.png?raw=1)"
      ],
      "metadata": {
        "id": "WDPOHq13JENe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-core langchain-community langchain-groq"
      ],
      "metadata": {
        "id": "OPiVCK-qTkGr",
        "outputId": "32172d2c-a482-4153-a9d7-23606fe1cfbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "llm_groq = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "uChGcceXTsJX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "class ClassificacaoNoticia(BaseModel):\n",
        "    emocao: Literal[\"Felicidade\", \"Tristeza\", \"Nojo\", \"Raiva\",\n",
        "                    \"Medo\", \"Surpreza\", \"Desprezo\"] = Field(\n",
        "                    description=\"A emoção primária transmitida pelo texto. Estamos usando como padrão as emoções universais de ekman\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=ClassificacaoNoticia)\n",
        "formato = parser.get_format_instructions()\n"
      ],
      "metadata": {
        "id": "OFNQQIZOTyKr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Classifique a notícia abaixo quanto a emoção.\n",
        "{format_instructions}\n",
        "Notícia: {noticia}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(\n",
        "    template=template,\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "aFjuHftWUmnm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_groq | parser\n",
        "chain.invoke(\"Nesse cenário de instabilidade, os investidores não sabem o que fazer.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX7wfKtEU_SL",
        "outputId": "07ca9e38-405c-4b76-b426-a7830caee48b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassificacaoNoticia(emocao='Medo')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def votacao(chain, texto: str, n: int) -> ClassificacaoNoticia:\n",
        "    votos = [chain.invoke(texto) for _ in range(n)]\n",
        "    vals = [voto.emocao for voto in votos]\n",
        "    conts = {}\n",
        "    for val in vals:\n",
        "        print(val)\n",
        "        if val in conts:\n",
        "            conts[val] += 1\n",
        "        else:\n",
        "            conts[val] = 1\n",
        "\n",
        "    return ClassificacaoNoticia(emocao=max(conts, key=conts.get))\n",
        "\n",
        "\n",
        "votacao(chain, \"Os resultados fiscais do primeiro semestre de 2024 indicam que, no curto prazo, a meta fiscal de -0,25% do PIB é mais viável, apesar das incertezas de médio e longo prazo devido aos desafios fiscais e ao aumento das despesas obrigatórias. O desempenho das receitas líquidas foi positivo, impulsionado pelas medidas legislativas de arrecadação, mas o crescimento das despesas, especialmente com benefícios previdenciários, elevou o déficit primário para R$ 68,7 bilhões, agravando o cenário fiscal. Apesar das medidas de contenção adotadas, a sustentabilidade das contas públicas a longo prazo permanece ameaçada, exigindo soluções concretas e uma postura cautelosa para os próximos anos.\", 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVA9DfWvUfOt",
        "outputId": "b7f5b8b3-bd93-4864-d045-e3fb71d3c208"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tristeza\n",
            "Tristeza\n",
            "Tristeza\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ClassificacaoNoticia(emocao='Tristeza')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Negociação"
      ],
      "metadata": {
        "id": "pAWUfO0TWrYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E se, em vez de votar de forma independente, os \"especialistas\" pudessem debater e refinar suas ideias? Essa é a premissa da Negociação, uma técnica que simula um processo de argumentação para resolver discordâncias. O trabalho de Sun et al. (2023) [2] explora essa ideia, mostrando que um debate estruturado pode ajudar a resolver ambiguidades.\n",
        "\n",
        "Vamos implementar um framework de debate iterativo com dois papéis:\n",
        "1. Gerador: Gera a análise e classificação inicial.\n",
        "2. Discriminador: Revisa a análise do propositor, busca falhas, pontos de vista alternativos ou ambiguidades e oferece uma contra-análise.\n",
        "\n",
        "O processo é um loop:\n",
        "1. O Gerador faz uma análise inicial.\n",
        "2. O Discriminador avalia a análise. Podendo concordar ou apresentar uma contra proposta.\n",
        "3. O Propositor revisa sua análise com base na crítica. E pode concordar ou enviar uma contra proposta.\n",
        "4. O ciclo se repete até um número máximo de rodadas ou até que um consenso seja alcançado."
      ],
      "metadata": {
        "id": "I-9IIHbhWwAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gerador = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "discriminador = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        "    temperature=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "3aIjooGSXfJ2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Negociacao(BaseModel):\n",
        "    raciocinio: str = Field(description=\"Linha de raciocínio passo a passo do modelo até chegar em uma decisão.\")\n",
        "    concordo: bool = Field(description=\"Representa se o modelo concorda ou não com o modelo anterior.\")\n",
        "    emocao: Literal[\"Felicidade\", \"Tristeza\", \"Nojo\", \"Raiva\",\n",
        "                    \"Medo\", \"Surpreza\", \"Desprezo\", \"Neutro\"] = Field(\n",
        "                    description=\"A emoção primária transmitida pelo texto. Estamos usando como padrão as emoções universais de ekman\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=Negociacao)\n",
        "formato = parser.get_format_instructions()"
      ],
      "metadata": {
        "id": "JwCg4cKgX_uH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gerador1_template = \"\"\"Classifique a notícia abaixo quanto a emoção.\n",
        "{format_instructions}\n",
        "Notícia: {noticia}\n",
        "\"\"\"\n",
        "\n",
        "gerador1_prompt = PromptTemplate.from_template(\n",
        "    template=gerador1_template,\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "BU3IYrTSX7NW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminador1_template = \"\"\"Você está em um debate com outro agente. A tarefa de vocês é classificar a notícia abaixo quanto a emoção.\n",
        "{format_instructions}\n",
        "Notícia: {noticia}\n",
        "Gerador: {gerador}\n",
        "\"\"\"\n",
        "\n",
        "discriminador1_prompt = PromptTemplate.from_template(\n",
        "    template=discriminador1_template,\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "TID39dN4YzFw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterações_template = \"\"\"Você está em um debate com outro agente. A tarefa de vocês é classificar a notícia abaixo quanto a emoção.\n",
        "{format_instructions}\n",
        "Notícia: {noticia}\n",
        "sua proposta: {analise}\n",
        "Contra proposta: {contra_proposta}\n",
        "\"\"\"\n",
        "\n",
        "iterações_prompt = PromptTemplate.from_template(\n",
        "    template=iterações_template,\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "YgyY6_weZAQW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inicio = gerador1_prompt | gerador | parser\n",
        "discriminar = discriminador1_prompt | discriminador | parser\n",
        "iterações1 = iterações_prompt | gerador | parser\n",
        "iterações2 = iterações_prompt | discriminador | parser"
      ],
      "metadata": {
        "id": "pafHW1-LZjAz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def negociacao(noticia: str, n: int = 3):\n",
        "    historico = [inicio.invoke({\"noticia\": noticia})]\n",
        "    historico.append(discriminar.invoke({\"noticia\": noticia, \"gerador\": historico[-1].raciocinio}))\n",
        "    for i in range(n-2):\n",
        "        if historico[-1].concordo:\n",
        "            break\n",
        "        if i % 2 == 0:\n",
        "            historico.append(iterações1.invoke({\"noticia\": noticia,\n",
        "                                                \"analise\": historico[-2].raciocinio,\n",
        "                                                \"contra_proposta\": historico[-1].raciocinio}))\n",
        "        else:\n",
        "            historico.append(iterações2.invoke({\"noticia\": noticia,\n",
        "                                                \"analise\": historico[-2].raciocinio,\n",
        "                                                \"contra_proposta\": historico[-1].raciocinio}))\n",
        "    return historico\n"
      ],
      "metadata": {
        "id": "XGTkIrx1ZUlw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historico = negociacao(\"Os resultados fiscais do primeiro semestre de 2024 indicam que, no curto prazo, a meta fiscal de -0,25% do PIB é mais viável, apesar das incertezas de médio e longo prazo devido aos desafios\")"
      ],
      "metadata": {
        "id": "rxXstJYwaNw7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, message in enumerate(historico):\n",
        "    print(f'{i + 1}° Iteração:')\n",
        "    role = 'Gerador' if i % 2 == 0 else 'Discriminador'\n",
        "    print(f'- {role}: {message.raciocinio}')\n",
        "    if i > 0:\n",
        "        print(f'- Concordo: {message.concordo}')\n",
        "    print(f'- Emoção: {message.emocao}')\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "M-HfO1SZDS77",
        "outputId": "e33469a9-4cee-4093-d815-db91086b5aa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1° Iteração:\n",
            "- Gerador: Os resultados fiscais do primeiro semestre de 2024 indicam que, no curto prazo, a meta fiscal de -0,25% do PIB é mais viável, apesar das incertezas de médio e longo prazo devido aos desafios\n",
            "- Emoção: Neutro\n",
            "\n",
            "2° Iteração:\n",
            "- Discriminador: A notícia apresenta dados positivos sobre os resultados fiscais do primeiro semestre de 2024, indicando que a meta fiscal de -0,25% do PIB é mais viável no curto prazo. No entanto, também menciona incertezas de médio e longo prazo devido a desafios, o que sugere uma perspectiva mais cautelosa.\n",
            "- Concordo: True\n",
            "- Emoção: Neutro\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Atividade Prática: Construindo um Ensemble Híbrido"
      ],
      "metadata": {
        "id": "uY2ZYzMaIos8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nas seções anteriores, implementamos a Votação Majoritária usando Self-Consistency (múltiplas chamadas ao mesmo modelo com o mesmo prompt). Agora, vamos elevar o nível e construir um sistema mais robusto: um Ensemble Híbrido.\n",
        "\n",
        "A ideia é aumentar a diversidade dos \"eleitores\". Em vez de perguntar a mesma coisa para a mesma pessoa várias vezes, vamos consultar diferentes especialistas (modelos), fazendo a eles diferentes tipos de perguntas (prompts) com a mesma questão.\n"
      ],
      "metadata": {
        "id": "urC0U9WiIm2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/Assaoka/Guide-to-Advanced-LLM-Techniques/blob/main/Imagens/HybridEnsemble.png?raw=1)"
      ],
      "metadata": {
        "id": "pBhF7eCDI29m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seu objetivo: Implementar uma função de votação para um ensemble híbrido que utiliza múltiplos modelos e múltiplos prompts para classificar uma notícia.\n",
        "\n",
        "Passos:\n",
        "1. Crie Prompts Variados: Crie dois PromptTemplate distintos para a tarefa de classificação de emoção.\n",
        "    - Prompt A (Direto): Pode ser similar ao que já usamos, pedindo a classificação de forma direta.\n",
        "    - Prompt B (Persona): Crie um prompt que instrua o LLM a agir com uma persona específica. Ex: \"Aja como um analista de risco cético e experiente. Analise a emoção primária que um investidor sentiria ao ler a seguinte notícia...\". Isso força o modelo a avaliar o texto de uma perspectiva diferente.\n",
        "2. Teste e Compare: Execute sua nova função com a notícia ambígua do \"Dilema do Investidor\". Compare o resultado e a diversidade dos votos com a abordagem de Self-Consistency. O resultado foi mais nuançado?"
      ],
      "metadata": {
        "id": "37-FDPOnI6JY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jbb9k70EJf-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências"
      ],
      "metadata": {
        "id": "bS3ogIYYJZ0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] F. Trad and A. Chehab, To Ensemble or Not: Assessing Majority Voting Strategies for Phishing Detection with Large Language Models. Springer Nature Switzerland, 2025, p. 158–173. [Online]. Available: http://dx.doi.org/10.1007/978-3-031-82150-9 13\n",
        "\n",
        "[2] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency improves chain of thought reasoning in language models,” 2023. [Online]. Available: https://arxiv.org/abs/2203.11171\n",
        "\n",
        "[3] X. Sun, X. Li, S. Zhang, S. Wang, F. Wu, J. Li, T. Zhang,  and G. Wang, “Sentiment analysis through llm negotiations,” 2023. [Online]. Available: https://arxiv.org/abs/2311.01876\n"
      ],
      "metadata": {
        "id": "ni1ucs_dJhDF"
      }
    }
  ]
}