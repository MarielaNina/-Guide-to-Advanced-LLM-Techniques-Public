{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarielaNina/-Guide-to-Advanced-LLM-Techniques-Public/blob/main/M%C3%B3dulo_4_Fine_Tuning_Eficiente_Especializando_seu_Pr%C3%B3prio_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 4: Fine-Tuning Eficiente - Especializando seu Próprio LLM"
      ],
      "metadata": {
        "id": "jsZHW92NtdxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução"
      ],
      "metadata": {
        "id": "SdEqY_fgtf8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Até agora, tratamos os LLMs como \"caixas-pretas\" que manipulamos através de entradas inteligentes (prompts) e combinações de saídas (ensembles). Neste último módulo, vamos abrir a caixa e modificar o próprio cérebro do modelo. O Fine-Tuning (ou ajuste fino) é o processo de continuar o treinamento de um modelo pré-treinado em um dataset específico de uma nova tarefa.\n",
        "\n",
        "Enquanto a engenharia de prompt adapta a tarefa ao modelo, o fine-tuning adapta o modelo à tarefa. Isso resulta em um modelo especialista, que não apenas entende as nuances da sua tarefa específica, mas também pode ser menor, mais rápido e mais barato de executar em produção.\n",
        "\n",
        "No entanto, o fine-tuning completo de um LLM com bilhões de parâmetros é computacionalmente proibitivo para a maioria. Por isso, este módulo foca em técnicas de Fine-Tuning Eficiente em Parâmetros (PEFT - Parameter-Efficient Fine-Tuning)."
      ],
      "metadata": {
        "id": "K3L_QF9otiC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. O que é Fine-Tuning?"
      ],
      "metadata": {
        "id": "sLkLltNHtpyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um LLM pré-treinado, como o Llama 3, aprendeu uma vasta gama de conhecimentos sobre linguagem, fatos e raciocínio a partir de trilhões de palavras da internet. O fine-tuning pega esse conhecimento geral e o especializa.\n",
        "\n",
        "O processo envolve apresentar ao modelo um dataset de exemplos para uma tarefa específica (ex: pares de \"notícia\" e \"sentimento\") e ajustar seus pesos internos através de backpropagation para minimizar o erro nessas novas predições. O resultado é um novo conjunto de pesos, criando um modelo que é um especialista na sua tarefa.\n",
        "\n",
        "**O principal desafio é o custo:** o fine-tuning completo de um modelo bilhões de parâmetros pode exigir GPUs de alta performance, algo inacessível para a maioria dos pesquisadores e desenvolvedores."
      ],
      "metadata": {
        "id": "YLln4EpktrH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. LoRA: Low-Rank Adaptation"
      ],
      "metadata": {
        "id": "60Cc0SVGuFb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para resolver o problema do custo, surgiram as técnicas PEFT (Parameter-Efficient Fine-Tuning). A mais popular delas é a LoRA (Low-Rank Adaptation), proposta por Hu et al. (2021) [1].\n",
        "\n",
        "A intuição por trás do LoRA é que a \"mudança\" que o fine-tuning provoca nos pesos de um modelo pré-treinado (W) tem uma \"baixa dimensão intrínseca\". Em vez de treinar a matriz de pesos W inteira, que é enorme, o LoRA congela W e treina apenas duas matrizes muito menores (A e B) que aproximam a matriz de atualização (ΔW). Durante a inferência, a atualização é simplesmente somada aos pesos originais.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AYBHwucuuG0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/blog/133_trl_peft/lora-animated.gif)"
      ],
      "metadata": {
        "id": "9ETDfp2-uhf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figura 1: Representação visual do LoRA. Os pesos pré-treinados (W) são congelados. Apenas as matrizes de baixa ordem (A e B) são treinadas, reduzindo drasticamente o número de parâmetros ajustáveis.\n",
        "\n",
        "Isso reduz o número de parâmetros treináveis drasticamente, tornando o fine-tuning muito mais rápido e com um consumo de memória menor."
      ],
      "metadata": {
        "id": "bf8P5f0buhBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Quantização de Modelos"
      ],
      "metadata": {
        "id": "doRTkOnTu3Cn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quantização ataca outro gargalo: o tamanho do modelo em memória. Por padrão, os pesos de um LLM são armazenados como números de ponto flutuante de alta precisão (ex: 32-bit). A quantização reduz essa precisão para 8-bit ou até 4-bit.\n",
        "\n",
        "Isso leva a um ponto importante: um modelo maior com pesos quantizados frequentemente supera um modelo menor com pesos de alta precisão, com um custo de hardware similar. A pesquisa em quantização extrema, como a proposta pela arquitetura BitNet que explora modelos de 1-bit (Wang et al., 2024) [3], mostra que a escala do modelo (mais parâmetros) muitas vezes compensa a perda de precisão de cada peso individual, mantendo um desempenho robusto com uma fração do custo de memória."
      ],
      "metadata": {
        "id": "8bom4jAvu-47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. QLoRA"
      ],
      "metadata": {
        "id": "JVOXmWRIvV_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA (Quantized Low-Rank Adaptation), de Dettmers et al. (2023) [2], é a combinação genial das duas técnicas anteriores e que democratizou o fine-tuning de LLMs.\n",
        "\n",
        "O processo é o seguinte:\n",
        "1. Carrega-se um LLM pré-treinado e o quantiza para uma precisão menor.\n",
        "2. Os pesos originais do modelo são congelados.\n",
        "3. Adiciona-se os adaptadores LoRA (A e B).\n",
        "4. Treina-se apenas os adaptadores LoRA.\n",
        "\n",
        "Essa técnica é o que vai nos permitir realizar o fine-tuning de um modelo como o Llama 3 em uma única GPU gratuita do Google Colab."
      ],
      "metadata": {
        "id": "SS2aKuy2vOuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8G5LfZ0tXBB"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers bitsandbytes accelerate peft trl datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Configurações do Colab"
      ],
      "metadata": {
        "id": "OH3NWPDxycLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mesmo com toda essa otimização de desempenho, não é possível executar no ambiente padrão do Colab. Siga os passos abaixo para se conectar a uma GPU gratuitamente:\n",
        "1. Clique com o botão esquerdo na seta ao lado do botão conectar que está localizado no canto superior direito.\n",
        "2. Clique em \"Alterar o tipo de ambiente de execução\".\n",
        "3. Selecione GPU\n",
        "4. Clique em Salvar\n"
      ],
      "metadata": {
        "id": "VDkd5V0Xygpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Hugging Face"
      ],
      "metadata": {
        "id": "IXdrD6XFv-B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para conseguir acesso ao modelo utilizaremos a biblioteca Hugging Face. Para isso siga o seguinte passo a passo:\n",
        "1. Crie sua conta no [Hugging Face](https://huggingface.co/)\n",
        "2. Solicite o [acesso](https://huggingface.co/meta-llama/Llama-3.2-1B) ao modelo"
      ],
      "metadata": {
        "id": "TX83NMPgwC8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "print(\"Por favor, insira seu token de acesso do Hugging Face:\")\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "2CiYq76PwmMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Carregando o Modelo e o Tokenizador"
      ],
      "metadata": {
        "id": "Oh0d32h_x1NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# Configuração do BitsAndBytes para quantização em 4-bit (QLoRA)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "M-MqmdspyHWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o modelo principal com a configuração de quantização\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "LLxD5XtE1qxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregando o tokenizador correspondente ao modelo\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # O pad_token é necessário para o SFTTrainer. Definimos como o token de fim de sequência (eos_token).\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "zoMpcBde1r0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Preparação do Dataset"
      ],
      "metadata": {
        "id": "HNzbOS5ozlpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esses modelos são treinados para tarefa de prever o próximo token. Dessa forma, precisamos converter nosso dataset em um texto que o modelo deve aprender a prever. Nosso objetivo vai ser criar um prompt no formato \"instrução -> resposta JSON\", para que o modelo aprenda as classificar as emoções e gerar saídas estruturadas."
      ],
      "metadata": {
        "id": "BWSYVJ_WzpQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Curso LLMs/dataset.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "WU0tzphP0cIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "template = \"\"\"Classifique a notícia abaixo conforme a polaridade e emoção universal de ekman.\n",
        "Texto:\n",
        "f1: {f1}\n",
        "f2: {f2}\n",
        "f3: {f3}\n",
        "\n",
        "Resultado:\n",
        "{{\n",
        "    'f1': {{\n",
        "        'polaridade': '{f1_polaridade}',\n",
        "        'emocao': '{f1_emocao}'\n",
        "    }},\n",
        "    'f2': {{\n",
        "        'polaridade': '{f2_polaridade}',\n",
        "        'emocao': '{f2_emocao}'\n",
        "    }},\n",
        "    'f3': {{\n",
        "        'polaridade': '{f3_polaridade}',\n",
        "        'emocao': '{f3_emocao}'\n",
        "    }}\n",
        "}}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template=template)"
      ],
      "metadata": {
        "id": "1C-1QAvL0hfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_json_prompt(row):\n",
        "    return {'text': prompt.invoke({\n",
        "        'f1': row['f1'],\n",
        "        'f1_polaridade': row['sentence1_polarity'],\n",
        "        'f1_emocao': row['sentence1_sentiment'],\n",
        "        'f2': row['f2'],\n",
        "        'f2_polaridade': row['sentence2_polarity'],\n",
        "        'f2_emocao': row['sentence2_sentiment'],\n",
        "        'f3': row['f3'],\n",
        "        'f3_polaridade': row['sentence3_polarity'],\n",
        "        'f3_emocao': row['sentence3_sentiment'],\n",
        "    }).text}"
      ],
      "metadata": {
        "id": "Toq4DQ_70_zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "hf_dataset = Dataset.from_pandas(df)\n",
        "formatted_dataset = hf_dataset.map(create_json_prompt)"
      ],
      "metadata": {
        "id": "v_puql0v1Chp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. Configuração de Treinamento"
      ],
      "metadata": {
        "id": "vfT0F6KN1MUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "h1wLpliG1X_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama3-finetuned-json\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=25,\n",
        "    learning_rate=4e-4,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=formatted_dataset,\n",
        "    peft_config=lora_config,\n",
        "    args=training_args,\n",
        ")"
      ],
      "metadata": {
        "id": "ElDiyQ701tIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6. Iniciar o Treinamento e Salvar"
      ],
      "metadata": {
        "id": "p-BqXmMt1-D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui será nescessário criar uma conta no wandb, na própria execução veem isntruções de como criar a conta."
      ],
      "metadata": {
        "id": "UnFIYI8u2cbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "o0Yzy99019qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./llama3-json-adapter\")"
      ],
      "metadata": {
        "id": "0FTJoc1p2xFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Integração com Langchain"
      ],
      "metadata": {
        "id": "OtxJNLnV2xp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain_core langchain_community langchain-huggingface"
      ],
      "metadata": {
        "id": "8FAPzrK83J0f",
        "outputId": "0ea50b76-b47b-4436-ec7c-0d9a46f944c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.5 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "TipoPolaridade = Literal['Positivo', 'Negativo', 'Neutro']\n",
        "TipoEmocao = Literal['Felicidade', 'Tristeza', 'Raiva', 'Nojo', 'Medo', 'Surpresa', 'Desprezo', 'Neutro']\n",
        "\n",
        "class AnaliseFrase(BaseModel):\n",
        "    \"\"\"Define a estrutura para a análise de uma única frase.\"\"\"\n",
        "    polaridade: TipoPolaridade = Field(description=\"A polaridade da frase (Positivo, Negativo ou Neutro)\")\n",
        "    emocao: TipoEmocao = Field(description=\"A emoção universal de Ekman detectada na frase\")\n",
        "\n",
        "class ClassificacaoNoticia(BaseModel):\n",
        "    \"\"\"Define a estrutura completa do JSON de saída para a notícia.\"\"\"\n",
        "    f1: AnaliseFrase = Field(description=\"Análise da primeira frase da notícia\")\n",
        "    f2: AnaliseFrase = Field(description=\"Análise da segunda frase da notícia\")\n",
        "    f3: AnaliseFrase = Field(description=\"Análise da terceira frase da notícia\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=ClassificacaoNoticia)"
      ],
      "metadata": {
        "id": "XWyYSHIs3QPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,             # Usando o modelo fine-tuned já carregado\n",
        "    tokenizer=tokenizer,     # Usando o tokenizador já carregado\n",
        "    max_new_tokens=250,      # Espaço suficiente para o JSON.\n",
        "    temperature=0.01,        # Temperatura baixa para saídas mais consistentes e determinísticas.\n",
        "    return_full_text=False,  # Importante: retorna apenas o texto gerado.\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "D0RCwFEk3acz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Classifique a noticia abaixo conforme a polaridade e emoção universal.\n",
        "Texto:\n",
        "f1: {f1_text}\n",
        "f2: {f2_text}\n",
        "f3: {f3_text}\n",
        "\n",
        "Resultado:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=template)\n",
        "\n",
        "chain = prompt | llm | parser"
      ],
      "metadata": {
        "id": "R1KFWN7_39lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = chain.invoke({\n",
        "    'f1_text': df.iloc[0]['f1'],\n",
        "    'f2_text': df.iloc[0]['f2'],\n",
        "    'f3_text': df.iloc[0]['f3'],\n",
        "})\n",
        "print(resposta)"
      ],
      "metadata": {
        "id": "BVzD5tCw4FSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Atividade Prática Final: Destilação de Conhecimento"
      ],
      "metadata": {
        "id": "Ey5h9JLH4IsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1. O que é Destilação?"
      ],
      "metadata": {
        "id": "AmP8-ecH4cfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A destilação de conhecimento é uma técnica de fine-tuning com um objetivo diferente. Em vez de apenas especializar um modelo, a ideia é usar um modelo grande e poderoso (o \"professor\") para treinar um modelo muito menor e mais eficiente (o \"aluno\").\n",
        "\n",
        "O processo é o seguinte:\n",
        "1. **Geração de Pseudo-Rótulos:** Usamos nosso melhor modelo \"professor\" (que pode ser um LLM de ponta via API, ou o melhor ensemble que criamos nos módulos anteriores) para classificar um grande volume de dados não rotulados. As saídas do professor são tratadas como se fossem rótulos verdadeiros.\n",
        "2. **Treinamento do Aluno:** Usamos esse dataset recém-rotulado para fazer o fine-tuning de um modelo \"aluno\" menor.\n",
        "\n",
        "O resultado é um modelo pequeno, rápido e barato de executar, que aprendeu a imitar o comportamento do professor para uma tarefa específica. Ele não terá o conhecimento geral do professor, mas será um especialista altamente eficiente na tarefa para a qual foi destilado."
      ],
      "metadata": {
        "id": "WdZ8lRsy4f8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2. Atividade"
      ],
      "metadata": {
        "id": "H3VYF4XE4zaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criar um classificador de sentimento pequeno e eficiente usando a técnica de destilação de conhecimento.\n",
        "\n",
        "Passos:\n",
        "1. **Escolha seu \"Professor\":** Use a sua implementação de Chain-of-Thought com saída estruturada do Módulo 2. O objetivo é que a saída do professor contenha não apenas a classificação final, mas também o raciocínio passo a passo.\n",
        "2. **Gere um Dataset:** Use o seu modelo professor para classificar o nosso dataset. A \"resposta\" no seu novo dataset de fine-tuning será um JSON contendo tanto o raciocinio quanto a classificacao.\n",
        "3. **Escolha seu \"Aluno\":** Selecione um modelo pequeno para ser o aluno.\n",
        "4. **Treine o Aluno para Pensar:** Adapte o código da seção 5 para fazer o fine-tuning do seu modelo aluno no dataset que você acabou de gerar. Ao fazer isso, você não está apenas ensinando o aluno a classificar, mas a imitar o processo de raciocínio do professor.\n",
        "5. **Avalie:** Compare o desempenho do seu novo modelo aluno com o do professor e com as abordagens dos módulos anteriores. O aluno treinado com raciocínio consegue um desempenho melhor do que o treinado apenas com rótulos?"
      ],
      "metadata": {
        "id": "R8aXDobB43k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusão do Tutorial"
      ],
      "metadata": {
        "id": "AhxlY-Kx5fu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parabéns por chegar ao final! Começamos com prompts simples, avançamos para debates complexos entre agentes e, finalmente, modificamos os próprios pesos de um LLM. Você agora possui um kit de ferramentas robusto para abordar qualquer tarefa de classificação de texto com LLMs."
      ],
      "metadata": {
        "id": "koaXIlfF5hOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências"
      ],
      "metadata": {
        "id": "sX30226l54aI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685\n",
        "\n",
        "[2] Dettmers, T., et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314\n",
        "\n",
        "[3] Wang, S., et al. (2024). The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits. arXiv:2402.17764"
      ],
      "metadata": {
        "id": "UwWI5S2t56po"
      }
    }
  ]
}