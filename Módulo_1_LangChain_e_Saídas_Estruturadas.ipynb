{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarielaNina/-Guide-to-Advanced-LLM-Techniques-Public/blob/main/M%C3%B3dulo_1_LangChain_e_Sa%C3%ADdas_Estruturadas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M√≥dulo 1: A Base - LangChain e Sa√≠das Estruturadas"
      ],
      "metadata": {
        "id": "ZvLjUp_fKE2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdu√ß√£o"
      ],
      "metadata": {
        "id": "EMf0dIDtKIRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bem-vindo ao primeiro m√≥dulo do nosso tutorial! O objetivo aqui √© construir a funda√ß√£o para todas as t√©cnicas avan√ßadas que exploraremos. Antes de mergulhar em engenharia de prompt, ensembles ou fine-tuning, precisamos garantir que conseguimos nos comunicar com os Modelos de Linguagem de Grande Porte (LLMs) de forma eficiente, robusta e, mais importante, program√°tica.\n",
        "\n",
        "Neste notebook, vamos abordar tr√™s pilares essenciais:\n",
        "1. **Configura√ß√£o do Ambiente:** Como acessar LLMs poderosos atrav√©s de APIs, com foco em op√ß√µes gratuitas.\n",
        "2. **Orquestra√ß√£o com LangChain:** Uma introdu√ß√£o √† biblioteca LangChain, que simplifica a cria√ß√£o de aplica√ß√µes com LLMs.\n",
        "3. **Gera√ß√£o de Sa√≠das Estruturadas:** A t√©cnica crucial para transformar as respostas em texto livre dos LLMs em formatos de dados consistentes e utiliz√°veis, como JSON, que s√£o a base para qualquer aplica√ß√£o real.\n",
        "\n",
        "Ao final deste m√≥dulo, voc√™ ter√° um ambiente configurado e ser√° capaz de instruir um LLM a retornar informa√ß√µes em um formato Python pr√©-definido, pronto para ser integrado em qualquer software."
      ],
      "metadata": {
        "id": "xAdELwFgKN07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Configura√ß√£o das APIs"
      ],
      "metadata": {
        "id": "reZmkDEkLAp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para interagir com a maioria dos LLMs de ponta, utilizamos uma Interface de Programa√ß√£o de Aplica√ß√µes (API). Ela funciona como uma \"ponte\" que permite que nosso c√≥digo envie requisi√ß√µes para o modelo e receba as respostas."
      ],
      "metadata": {
        "id": "v9TAzMqnLDC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Groq"
      ],
      "metadata": {
        "id": "hSP97LKoLEFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Groq √© uma plataforma de infer√™ncia de IA de alta performance. Ela oferece acesso via API a diversos modelos open-source de ponta.\n",
        "\n",
        "Ela possui 2 atrativos principais. O primeiro √© que ela oferece um plano gratuito que permite rodar LLMs poderosos de forma gratuita (dentro de certos limites de uso). O segundo √© a velocidade de processamento, que √© extremamente r√°pida.\n",
        "\n",
        "Voc√™ pode criar sua conta gratuitamente ([link](console.groq.com)) e, ap√≥s o login, gerar sua chave de API na aba ‚ÄúAPI Keys‚Äù. Depois de se cadastrar, siga o seguinte passo a passo:\n",
        "1. Acesse a aba **Secrets** (representada por um √≠cone de chave na barra lateral do Colab)\n",
        "2. Clique em **\"Adicionar novo secret\"**\n",
        "3. Defina o Nome `\"GROQ_API_KEY\"`\n",
        "4. Cole a chave de API que voc√™ gerou na plataforma\n",
        "\n",
        "Neste tutorial, vamos utilizar a API da Groq, mas voce pode usar qualquer outro modelo com integra√ß√£o com o langchain."
      ],
      "metadata": {
        "id": "bQGQ2VtMLK8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aTlUIKYkKBMt"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain langchain-core langchain-community langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "llm_groq = ChatGroq(\n",
        "    model=\"deepseek-r1-distill-llama-70b\",\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        "    temperature=0.7\n",
        ")"
      ],
      "metadata": {
        "id": "e2DA4UxyOJL4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Sabi√° / Maritaca.AI"
      ],
      "metadata": {
        "id": "W7oEYi4IOTNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "√â uma plataforma brasileira que disponibiliza modelos de linguagem treinados em portugu√™s com uma API compat√≠vel com a da OpenAI. Ao se cadastrar no servi√ßo [(link)](https://www.maritaca.ai/), voc√™ ganha R$20,00 de cr√©dito gr√°tis ap√≥s verificar um m√©todo de pagamento.\n",
        "\n",
        "Esse cr√©dito inicial √© suficiente para realizar o tutorial. Opcionalmente, voc√™ pode fazer uma pequena recarga (mesmo o valor m√≠nimo) porque isso aumenta a velocidade de gera√ß√£o e eleva o limite de requisi√ß√µes por minuto consideravelmente. N√£o √© obrigat√≥rio para seguir o tutorial, mas se voc√™ notar lentid√£o, essa pode ser uma solu√ß√£o.\n",
        "\n",
        "Por ser compat√≠vel com a API da OpenAI, usar o Sabi√° √© muito f√°cil: voc√™ pode utilizar bibliotecas pensadas para OpenAI apenas substituindo a chave de API e endpoint. Al√©m disso, o Sabi√° j√° suporta funcionalidades avan√ßadas como sa√≠das estruturadas e chamada de fun√ß√£o de forma nativa. Ou seja, √© uma alternativa local/nacional para usar LLMs poderosos a custo menor e com suporte ao portugu√™s.\n",
        "\n",
        "Depois de se cadastrar, adicione a chave `MARITALK_API_KEY` √° aba Secrets do Colab seguindo os mesmos passos do Groq."
      ],
      "metadata": {
        "id": "5PcTfXwZOUZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatMaritalk\n",
        "\n",
        "llm_sabia = ChatMaritalk(\n",
        "    model=\"sabiazinho-3\",\n",
        "    api_key=userdata.get('MARITALK_API_KEY'),\n",
        "    temperature=0.7,\n",
        ")"
      ],
      "metadata": {
        "id": "Ru5qPzbgO40z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Introdu√ß√£o ao LangChain"
      ],
      "metadata": {
        "id": "eOaTSXqNPVa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Interagindo com o Modelo"
      ],
      "metadata": {
        "id": "HC3FanWVPZ5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construir aplica√ß√µes com LLMs envolve mais do que apenas enviar um prompt para uma API. Frequentemente, precisamos encadear m√∫ltiplas chamadas, gerenciar o hist√≥rico da conversa, conectar o LLM a fontes de dados externas e formatar suas sa√≠das. Fazer tudo isso manualmente pode ser complexo e repetitivo.\n",
        "\n",
        "√â aqui que entra o LangChain [1]. LangChain √© um framework de c√≥digo aberto projetado para simplificar o desenvolvimento de aplica√ß√µes baseadas em LLMs. Ele fornece um conjunto de abstra√ß√µes e componentes modulares que facilitam a cria√ß√£o de cadeias (chains) e agentes complexos.\n",
        "\n",
        "J√° vimos como instanciar ambos os modelos, para chamar eles no nosso c√≥digo bastam chamar o m√©todo invoke com uma string."
      ],
      "metadata": {
        "id": "tBilnLQgPqHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resposta = llm_groq.invoke(\"Ol√°, tudo bem?\")\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzh7UErcPXHZ",
        "outputId": "450ddffb-1a1a-4229-995f-6febc1322b64"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='<think>\\n\\n</think>\\n\\nOl√°! Sim, estou bem, obrigado. Como posso ajudar voc√™ hoje? üòä' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 9, 'total_tokens': 36, 'completion_time': 0.096885119, 'prompt_time': 0.010130248, 'queue_time': 2.477559818, 'total_time': 0.107015367}, 'model_name': 'deepseek-r1-distill-llama-70b', 'system_fingerprint': 'fp_e98d30d035', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None} id='run--51beb9ed-7ab7-44e4-8fa2-2e6ffb8765cf-0' usage_metadata={'input_tokens': 9, 'output_tokens': 27, 'total_tokens': 36}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com apenas uma linha de c√≥digo, podemos interagir com as llms, com toda a complexidade da comunica√ß√£o via API abstra√≠da pelo LangChain."
      ],
      "metadata": {
        "id": "WoIh0owQP33W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Generaliza√ß√£o de Prompts\n",
        "O langchain tamb√©m oferece uma forma de criar prompts din√¢micos reutilizaveis, fazemos isso atrav√©s do prompt template."
      ],
      "metadata": {
        "id": "QAP3q8WjP8dW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"Escreva um poema {tamanho} sobre {tema}.\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpEZP4u8P4lZ",
        "outputId": "f1c7bd39-6e16-4e69-b9b0-d9e8891b956b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['tamanho', 'tema'], input_types={}, partial_variables={}, template='Escreva um poema {tamanho} sobre {tema}.')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A maioria das intera√ß√µes atrav√©s do langchain acontece atrav√©s do m√©todo `invoke`. Podemos gerar o prompt para a LLM da seguinte forma:"
      ],
      "metadata": {
        "id": "Ywv6MLFHQWMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poema = prompt.invoke({\"tamanho\": \"curto\", \"tema\": \"a lua\"})\n",
        "poema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JibheUiQQZf1",
        "outputId": "ce3ad5ae-08f4-461b-d9c4-9203ea070403"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='Escreva um poema curto sobre a lua.')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Cadeias (Chains)"
      ],
      "metadata": {
        "id": "iMoY66avQpLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que temos um prompt din√¢mico e um llm, podemos combin√°-los. A forma mais idiom√°tica e poderosa de fazer isso no LangChain √© atrav√©s de `chains`, utilizando o operador pipe (`|`). Isso cria um fluxo de dados leg√≠vel e modular, onde a sa√≠da de um componente se torna automaticamente a entrada do pr√≥ximo, simplificando a l√≥gica."
      ],
      "metadata": {
        "id": "xn3DqAuQQsdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser"
      ],
      "metadata": {
        "id": "dOJBEgJqRaD-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entrada = prompt.invoke({\"tamanho\": \"curto\", \"tema\": \"a lua\"})\n",
        "resposta = llm_groq.invoke(entrada)\n",
        "\n",
        "parser = StrOutputParser()\n",
        "resposta = parser.invoke(resposta)\n",
        "\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI_z15vWQy6Q",
        "outputId": "173ba877-9e89-4fdf-fd2a-ac48ba19c7da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, I need to write a short poem about the moon. Let me think about what the moon represents. It's often associated with night, beauty, mystery, and sometimes loneliness. I should use some imagery that evokes these feelings.\n",
            "\n",
            "Maybe start with the moon in the sky, its glow. I can describe its color, like silver or white. Then, perhaps talk about its reflection on water, which is a common poetic image.\n",
            "\n",
            "I should also consider the phases of the moon, maybe mention crescent or full moon. This adds variety to the poem. I can talk about how the moon affects the night, maybe personify it a bit, giving it actions or emotions.\n",
            "\n",
            "Rhyme scheme is important. Let's go with something simple, like ABAB or AABB. Each stanza can have four lines. I'll keep the language simple and flowing, avoiding complicated words so it's easy to understand and has a nice rhythm.\n",
            "\n",
            "Let me brainstorm some lines. First stanza: Introduce the moon in the sky, its glow, maybe its silence. Second stanza: Its reflection, the night's softness, perhaps the world's quiet. Third stanza: Different phases, maybe some metaphor like a mirror or boat. Fourth stanza: The moon's song or whisper, ending with a gentle image.\n",
            "\n",
            "Now, putting it all together, ensuring each line flows into the next and the rhyme is consistent. I'll check each stanza for rhythm and imagery, making sure each part contributes to the overall mood of the poem.\n",
            "\n",
            "Finally, I'll review the poem to make sure it's concise, captures the essence of the moon, and has a pleasant, lyrical quality.\n",
            "</think>\n",
            "\n",
            "**Lua Cheia**\n",
            "\n",
            "No c√©u azul, um globo branco,  \n",
            "A lua cheia, serena,  \n",
            "Em sil√™ncio, seu brilho espalha  \n",
            "Sobre a terra, uma cena.  \n",
            "\n",
            "Seu reflexo no mar,  \n",
            "Um caminho de luz, atraente,  \n",
            "A noite suave, sem igual,  \n",
            "O mundo calado, consentente.  \n",
            "\n",
            "Fase nova, crescente,  \n",
            "Em cada etapa, um encanto,  \n",
            "No escuro, seu brilho radiante,  \n",
            "Ilumina o sonho, o canto.  \n",
            "\n",
            "Cantiga suave, seu sussurro,  \n",
            "Na noite quieta, ecoa,  \n",
            "Lua cheia, puro encanto,  \n",
            "Na alma, paz, seduz.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_groq | StrOutputParser()\n",
        "resposta = chain.invoke({\"tamanho\": \"curto\", \"tema\": \"a lua\"})\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFGagp8NRJOA",
        "outputId": "d58a147a-bb4c-4798-b506-8b95bf2df87b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, I need to write a short poem about the moon. Hmm, where do I start? Well, the moon is such a beautiful and timeless subject. I should think about its characteristics. It's often associated with night, light, and maybe a bit of mystery.\n",
            "\n",
            "Let me brainstorm some words related to the moon: glowing, silver, crescent, full moon, phases, night, sky, stars, light, shadow, dream, silent, beacon. That should give me a good foundation.\n",
            "\n",
            "Now, I want the poem to flow well, so maybe I can structure it in a few stanzas. Let's see, perhaps start with describing the moon in the sky, then move on to its phases and its effect on the night.\n",
            "\n",
            "I should also think about the rhythm and rhyme. Maybe a simple AABB rhyme scheme would work well for a short poem. Each stanza can have four lines, with the second and fourth lines rhyming.\n",
            "\n",
            "First stanza: Introduce the moon in the sky. Something like \"The moon ascends the velvet sky,\" and then another line about its glow. Maybe \"With gentle glow, it catches the eye.\" That rhymes and sets a nice scene.\n",
            "\n",
            "Second stanza: Talk about the crescent and full moon. \"A crescent smile or full-faced light,\" and then something about its phases. \"It dances through the phases of night.\" That keeps the rhythm and continues the theme.\n",
            "\n",
            "Third stanza: Maybe personify the moon a bit, giving it some action. \"Silent observer of the world below,\" and then \"A beacon in the dark, it gently flows.\" That adds depth and emotion.\n",
            "\n",
            "Fourth stanza: Connect the moon to the stars and night. \"Among the stars, it finds its place,\" and then \"A guiding force in time and space.\" That gives it a sense of purpose.\n",
            "\n",
            "Fifth stanza: Conclude with the moon's presence in our lives. \"The moon's soft light upon us shines,\" and end with \"A constant friend through all our lines.\" That ties it all together nicely.\n",
            "\n",
            "Wait, let me check the syllables to make sure each line flows well. The first line has eight syllables, the second eight as well. That should make it consistent. Let me read it out loud to check the rhythm. It seems smooth and has a nice cadence.\n",
            "\n",
            "I think that covers the main aspects of the moon: its appearance, phases, role in the night, and its connection to us. The poem is short, each stanza is four lines, and it follows a rhyme scheme. I'm happy with how it turned out!\n",
            "</think>\n",
            "\n",
            "The moon ascends the velvet sky,  \n",
            "With gentle glow, it catches the eye.  \n",
            "A crescent smile or full-faced light,  \n",
            "It dances through the phases of night.  \n",
            "\n",
            "Silent observer of the world below,  \n",
            "A beacon in the dark, it gently flows.  \n",
            "Among the stars, it finds its place,  \n",
            "A guiding force in time and space.  \n",
            "\n",
            "The moon's soft light upon us shines,  \n",
            "A constant friend through all our lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Sa√≠das Estruturadas"
      ],
      "metadata": {
        "id": "ICAepivkR4pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.¬†A Necessidade de Sa√≠das Estruturadas"
      ],
      "metadata": {
        "id": "PYdITDpwR8Dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um dos maiores desafios ao usar LLMs em aplica√ß√µes de software √© que, por natureza, eles geram texto n√£o estruturado. Para uma tarefa de classifica√ß√£o de sentimento, um LLM pode responder:\n",
        "1. \"O sentimento √© positivo\"\n",
        "2. \"Positivo\"\n",
        "3. \"Com base na an√°lise, o texto expressa um sentimento positivo.\"\n",
        "4. ...\n",
        "\n",
        "Todas essas respostas s√£o corretas para um humano, mas a varia√ß√£o torna dif√≠cil para um programa process√°-las de forma confi√°vel.\n",
        "\n",
        "Quando pedimos algo a um modelo de linguagem, por padr√£o ele retorna texto livre, em linguagem natural, o que muitas vezes √© suficiente para uma conversa ou resposta direta. No entanto, em aplica√ß√µes pr√°ticas, frequentemente queremos que o modelo nos d√™ a resposta em um formato espec√≠fico e estruturado para podermos process√°-la automaticamente. Exemplos comuns:\n",
        "- Preencher campos de um formul√°rio ou banco de dados (por exemplo, extrair de um texto o {\"nome\": ..., \"email\": ..., \"telefone\": ...} em formato JSON).\n",
        "- Listar informa√ß√µes em forma de tabela (CSV) ou em bullet points bem definidos.\n",
        "- Retornar um conjunto de pares chave-valor, XML ou outro formato que alguma outra parte do sistema espera.\n",
        "\n",
        "Ter um formato de sa√≠da estruturado torna a integra√ß√£o entre LLMs e sistemas tradicionais muito mais confi√°vel. Se um modelo responde com um par√°grafo de texto explicativo, √© dif√≠cil para um programa extrair exatamente as partes relevantes sem risco de erro. Por outro lado, se conseguimos fazer o modelo responder, por exemplo, em JSON com campos definidos, podemos diretamente carregar esse JSON em uma estrutura de dados na nossa aplica√ß√£o (um dicion√°rio Python, por exemplo) e utilizar as informa√ß√µes de forma determin√≠stica.\n",
        "\n",
        "$$\\text{estrutura} = \\text{automa√ß√£o simplificada}.$$"
      ],
      "metadata": {
        "id": "b4vQl-CVSB6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Instru√ß√£o via Prompt"
      ],
      "metadata": {
        "id": "297Px6usS3JX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A abordagem mais direta para obter uma sa√≠da estruturada √© simplesmente pedir explicitamente no prompt que o modelo formate a resposta de determinada forma. Por exemplo, podemos acrescentar √†s instru√ß√µes algo como: \"Responda somente no formato JSON, contendo as seguintes chaves...\". Essa t√©cnica de prompt engineering muitas vezes resolve casos simples."
      ],
      "metadata": {
        "id": "bDPaGBBrS7RW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Extraia as seguintes informa√ß√µes do curr√≠culo abaixo:\n",
        "- Nome\n",
        "- Forma√ß√£o\n",
        "- Anos de experi√™ncia\n",
        "\n",
        "Formate a sa√≠da em JSON:\n",
        "{{\n",
        "    \"nome\": \"Nome do candidato\",\n",
        "    \"formacao\": \"Forma√ß√£o do candidato\",\n",
        "    \"anos_experiencia\": \"Anos de experi√™ncia do candidato\"\n",
        "}}\n",
        "\n",
        "Curr√≠culo:\"{resume_text}\"\n",
        "\n",
        "JSON:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"resume_text\"]\n",
        ")"
      ],
      "metadata": {
        "id": "2o4svMe5R57f"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_groq | StrOutputParser()\n",
        "resume = \"Meu nome √© Mariela Nina, tenho 24 anos. Sou Bacharel em Ci√™ncia da Computa√ß√£o pela Universidade Federal de Minas Gerais (UFMG). Fui Desenvolvedor de Software na Empresa X por 5 anos e Gestor de Projetos na Empresa Y por 3 anos onde trabalho atualmente.\"\n",
        "\n",
        "resposta = chain.invoke({\"resume_text\": resume})\n",
        "print(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O_H2iRiTMRt",
        "outputId": "3e8ba032-8c40-4a9d-e54e-362824fdcfb4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Ok, estou vendo o pedido do usu√°rio. Ele quer que eu extraia informa√ß√µes espec√≠ficas de um curr√≠culo fornecido. As informa√ß√µes necess√°rias s√£o nome, forma√ß√£o e anos de experi√™ncia, e formatar em JSON.\n",
            "\n",
            "Primeiro, vou ler o curr√≠culo cuidadosamente. O nome est√° logo no in√≠cio: \"Mariela Nina\". √ìtimo, isso √© direto.\n",
            "\n",
            "Em seguida, a forma√ß√£o. Vejo que ela √© Bacharel em Ci√™ncia da Computa√ß√£o e menciona a Universidade Federal de Minas Gerais, UFMG. Vou anotar isso.\n",
            "\n",
            "Agora, anos de experi√™ncia. Ela trabalhou como Desenvolvedor de Software na Empresa X por 5 anos e como Gestor de Projetos na Empresa Y por 3 anos. No total, s√£o 5 + 3 = 8 anos. Vou somar esses per√≠odos.\n",
            "\n",
            "Preciso garantir que o JSON esteja formatado corretamente. Vou organizar as chaves: nome, formacao e anos_experiencia. Certificar que os valores estejam entre aspas e que o JSON inteiro esteja bem estruturado.\n",
            "\n",
            "Vou revisar para garantir que n√£o haja erros de digita√ß√£o ou de c√°lculo. Nome: Mariela Nina. Forma√ß√£o: Bacharel em Ci√™ncia da Computa√ß√£o pela UFMG. Anos de experi√™ncia: 8 anos.\n",
            "\n",
            "Finalmente, vou montar o JSON com essas informa√ß√µes e apresentar ao usu√°rio.\n",
            "</think>\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"nome\": \"Mariela Nina\",\n",
            "    \"formacao\": \"Bacharel em Ci√™ncia da Computa√ß√£o pela Universidade Federal de Minas Gerais (UFMG)\",\n",
            "    \"anos_experiencia\": 8\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe que ainda recebemos informa√ß√µes fora da estrutura (pensamento no caso do DeepSeek), por√©m, o tratamento desse texto √© bem mais simples do que a sa√≠da comum."
      ],
      "metadata": {
        "id": "wKKiSrE3Ti0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Convers√£o para Objetos"
      ],
      "metadata": {
        "id": "TLH54eSeUAl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain oferece uma maneira elegante de implementar sa√≠das estruturadas usando a biblioteca Pydantic. Pydantic permite definir a estrutura dos seus dados usando classes Python. LangChain, ent√£o, usa essa defini√ß√£o para:\n",
        "1. Gerar automaticamente uma instru√ß√£o de formata√ß√£o para o LLM.\n",
        "2. Analisar a sa√≠da de texto do LLM e convert√™-la em um objeto Python.\n",
        "\n",
        "O Pydantic √© uma biblioteca Python para valida√ß√£o de dados e gerenciamento de configura√ß√µes. Ela permite definir a estrutura de dados desejada usando classes Python normais, com tipos de dados for√ßados. Para nosso prop√≥sito, sua principal vantagem √© a capacidade de criar 'modelos de dados' que o LangChain pode usar para instruir o LLM sobre como formatar sua sa√≠da e, em seguida, validar se a sa√≠da do modelo corresponde a essa estrutura."
      ],
      "metadata": {
        "id": "yu1uUP19UB4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "\n",
        "class InfoCurriculo(BaseModel):\n",
        "    nome: str = Field(description=\"Nome do candidato\")\n",
        "    formacao: str = Field(description=\"Forma√ß√£o do candidato\")\n",
        "    anos_experiencia: float = Field(description=\"Anos de experi√™ncia do candidato\")\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=InfoCurriculo)\n",
        "formato = parser.get_format_instructions()\n",
        "print(formato)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9RjoXt5Ua1A",
        "outputId": "b60d0ebf-761e-4891-c0e1-c283434d07c8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"nome\": {\"description\": \"Nome do candidato\", \"title\": \"Nome\", \"type\": \"string\"}, \"formacao\": {\"description\": \"Forma√ß√£o do candidato\", \"title\": \"Formacao\", \"type\": \"string\"}, \"anos_experiencia\": {\"description\": \"Anos de experi√™ncia do candidato\", \"title\": \"Anos Experiencia\", \"type\": \"number\"}}, \"required\": [\"nome\", \"formacao\", \"anos_experiencia\"]}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Extraia as seguintes informa√ß√µes do curr√≠culo abaixo:\n",
        "- Nome\n",
        "- Forma√ß√£o\n",
        "- Anos de experi√™ncia\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Curr√≠culo: \"{resume_text}\"\n",
        "\n",
        "JSON:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"resume_text\"],\n",
        "    partial_variables={\"format_instructions\": formato}\n",
        ")"
      ],
      "metadata": {
        "id": "ZgZa_MAsT-HW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_groq | parser\n",
        "resposta = chain.invoke({\"resume_text\": resume})\n",
        "resposta\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VmE-a6nUtwu",
        "outputId": "127a3d4b-3b3b-40c8-8ab3-909523e26b9d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InfoCurriculo(nome='Mariela Nina', formacao='Bacharel em Ci√™ncia da Computa√ß√£o pela Universidade Federal de Minas Gerais (UFMG)', anos_experiencia=8.0)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposta.nome"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_uq7RZB_U15z",
        "outputId": "e5bd5dd7-f41f-47c7-a476-12df87bd5305"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mariela Nina'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Abordagem Multi-LLM"
      ],
      "metadata": {
        "id": "6GHsy4isWi6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embora for√ßar a formata√ß√£o em um √∫nico passo seja conveniente, a complexidade de realizar duas tarefas simultaneamente ‚Äî extrair a informa√ß√£o e format√°-la perfeitamente ‚Äî pode, por vezes, degradar a qualidade do resultado. Para mitigar isso, podemos decompor o problema: uma primeira chamada ao LLM foca apenas em extrair a informa√ß√£o em linguagem natural, e uma segunda chamada, que pode usar um modelo mais simples e r√°pido, foca exclusivamente em converter essa extra√ß√£o para o formato JSON desejado."
      ],
      "metadata": {
        "id": "hm8ZxiXBWsfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_extracao = \"\"\"Extraia as seguintes informa√ß√µes do curr√≠culo abaixo:\n",
        "- Nome\n",
        "- Forma√ß√£o\n",
        "- Anos de experi√™ncia\n",
        "\n",
        "Apresente as informa√ß√µes extra√≠das de forma clara.\n",
        "\n",
        "Curr√≠culo:\n",
        "{curriculo}\n",
        "\"\"\"\n",
        "prompt_extracao = PromptTemplate.from_template(template_extracao)\n",
        "\n",
        "chain_extracao = prompt_extracao | llm_groq | StrOutputParser()"
      ],
      "metadata": {
        "id": "00_qWkCpWkBh"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_formatacao = \"\"\"\n",
        "Formate a informa√ß√£o extra√≠da abaixo para um objeto JSON.\n",
        "Siga estritamente o esquema JSON fornecido.\n",
        "\n",
        "Informa√ß√£o Extra√≠da:\n",
        "{info_extraida}\n",
        "\n",
        "Esquema JSON:\n",
        "{esquema_json}\n",
        "\"\"\"\n",
        "prompt_formatacao = PromptTemplate.from_template(template_formatacao)\n",
        "\n",
        "# Modelo LLM para a segunda etapa (pode ser um modelo mais r√°pido/barato).\n",
        "llm_formatador = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    api_key=userdata.get('GROQ_API_KEY'),\n",
        "    temperature=0.0 # Temperatura 0 para a tarefa de formata√ß√£o, que deve ser determin√≠stica.\n",
        ")\n"
      ],
      "metadata": {
        "id": "M8XysxN6XHyD"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_multi_llm = (\n",
        "    {\n",
        "        \"info_extraida\": chain_extracao,\n",
        "        \"esquema_json\": lambda x: formato\n",
        "    }\n",
        "    | prompt_formatacao\n",
        "    | llm_formatador\n",
        "    | parser\n",
        ")\n",
        "\n",
        "resposta = chain_multi_llm.invoke(resume)\n",
        "resposta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCfQ0nKRXV6G",
        "outputId": "57ebe3f6-9dfc-4154-df00-49b415523b62"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InfoCurriculo(nome='Mariela Nina', formacao='Bacharel em Ci√™ncia da Computa√ß√£o pela Universidade Federal de Minas Gerais (UFMG)', anos_experiencia=8.0)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Atividade Pr√°tica: Classificador de Not√≠cias Financeiras"
      ],
      "metadata": {
        "id": "fc-FGLe9YSXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora √© sua vez de aplicar o que aprendeu! Vamos usar um pequeno conjunto de dados de not√≠cias financeiras para treinar suas habilidades.\n",
        "\n",
        "**Seu objetivo:** Criar uma chain que recebe uma frase de uma not√≠cia e retorna um objeto estruturado com sua polaridade e emocao.\n",
        "\n",
        "Passos:\n",
        "1. **Defina a Estrutura:** Crie uma classe Pydantic chamada ClassificacaoNoticia com os campos nescess√°rios.\n",
        "    - O dataset cont√©m not√≠cias financeiras em portugu√™s.\n",
        "    - Cada not√≠cia foi resumida em 3 frases, que representam come√ßo (f1), meio (f2) e fim (f3).\n",
        "    - Cada frase possui uma polaridade (positivo, neutro, negativo) e uma emo√ß√£o universal do ekman (felicidade, tristeza, raiva, nojo, medo, surpreza e desprezo)\n",
        "2. **Crie o Parser:** Instancie um PydanticOutputParser com base na sua classe.\n",
        "3. **Crie o Prompt:** Elabore um PromptTemplate que instrua o LLM a classificar o sentimento de um texto de not√≠cia, usando as instru√ß√µes de formato do seu parser.\n",
        "4. **Construa a Chain:** Utilize a abordagem Multi-LLM com convers√£o para objetos com uma √∫nica chamada.\n",
        "5. **Teste:** Execute sua implementa√ß√£o para alguns elementos do dataset e imprima os resultados."
      ],
      "metadata": {
        "id": "kB-l8FZJYbzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Curso LLMs/dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WDcUn7KwamvR",
        "outputId": "054768d0-c1ff-4912-ff30-39518fa18172"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Curso LLMs/dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2154356664.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Curso LLMs/dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Curso LLMs/dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificacaoNoticia(BaseModel):\n",
        "    pass"
      ],
      "metadata": {
        "id": "JWDQdOHAYV6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclus√£o do M√≥dulo"
      ],
      "metadata": {
        "id": "AL6KpDe-a0cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parab√©ns! Voc√™ configurou seu ambiente, aprendeu a usar o LangChain para interagir com um LLM e, o mais importante, implementou uma forma robusta de obter sa√≠das estruturadas. O objeto `ClassificacaoSentimento` que recebemos no final √© previs√≠vel e f√°cil de usar em qualquer sistema.\n",
        "\n",
        "Essa habilidade √© o alicerce sobre o qual construiremos t√©cnicas mais sofisticadas. No pr√≥ximo m√≥dulo, vamos nos aprofundar na Engenharia de Prompt para melhorar drasticamente a qualidade e a precis√£o das respostas do modelo."
      ],
      "metadata": {
        "id": "99oH2iY2a4Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refer√™ncias"
      ],
      "metadata": {
        "id": "pnAFALv-bFvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] Chase, H. (2022). LangChain. GitHub. https://github.com/langchain-ai/langchain"
      ],
      "metadata": {
        "id": "5YooXhEKbH5Z"
      }
    }
  ]
}